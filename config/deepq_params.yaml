alpha: 0.01 # Taxa de Aprendizagem
alpha_decay: 0.01 # Taxa de Decaimento
gamma: 1.0 # recompensas futuras valor 0 nenhum 1 muito
epsilon: 1.0 # exploração, 0 nenhum 1 muito
epsilon_decay: 0.995 # como reduzimos a exploração
epsilon_min: 0.01 # valor mínimo que o epsilon pode ter
batch_size: 64 # tamanho máximo dos lotes amostrados da memória
episodes_training: 1000
n_win_ticks: 250 # Se a média de recompensas for maior do que isso e tiver passado min_episodes, a tarefa é considerada concluída
min_episodes: 100
#max_env_steps: None
monitor: True # armazena os resultados do openai gym em um arquivo
quiet: False # Usado para mostrar mensagens na tela


# Siga a linha Parâmetros relacionados ao ambiente de tarefas
number_decimals_precision_obs: 4
speed_step: 1.0 # Time to wait in the reset phases

linear_forward_speed: 1.5 # Gerado para ir para a frente
linear_turn_speed: 0.1 # Velocidade linear ao virar
angular_speed: 0.45 # Velocidade angular ao virar à esquerda ou à direita
init_linear_forward_speed: 0.0 # Velocidade linear inicial em que começamos cada episódio
init_linear_turn_speed: 0.0 # Velocidade angular inicial em que começamos cada episódio

n_observations: 19200 # (80, 80, 3) # Número de pixels usados como observações
n_actions: 3 # Número de ações usadas pelo algoritmo e tarefa

# Quantos metros à frente das leituras do fluxo da câmera que usamos para detecção, quanto menor, menos rápido o processamento
look_ahead_distance: 100
cur_lane_offset: 1.51
max_lane_offset: 1.5 # Valor considerado máximo, lado extremo da pista em relação ao centro. Cx = 320 = came de largura
safe_lane_offset: -1.5 # Valor considerado nominal, próximo ao centro da pista. Cx = 0 => Está no lado esquerdo máximo

follow_lane_reward: # 5 Nº de pontos dados para seguir a pista
left_right_reward: 2.5 # Pontos dados quando uma ação de turno é realizada
veer_off_reward: -10
end_episode_points: 200 # Pontos dados ao terminar um episódio